{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from geopy.distance import geodesic\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Calculate distance from home function\n",
    "def calculate_distance(row):\n",
    "    home_location = (row['lat'], row['long'])\n",
    "    merch_location = (row['merch_lat'], row['merch_long'])\n",
    "    return geodesic(home_location, merch_location).miles\n",
    "\n",
    "# Function to calculate distance between two points\n",
    "def calculate_distance2(row1, row2):\n",
    "    point1 = (row1['lat'], row1['long'])\n",
    "    point2 = (row2['lat'], row2['long'])\n",
    "    return geodesic(point1, point2).miles\n",
    "\n",
    "def calculate_similarity_score(amount, fraud_mean, fraud_std, normal_mean, normal_std):\n",
    "    # Calculate Z-scores for fraud and normal\n",
    "    z_score_fraud = abs((amount - fraud_mean) / fraud_std)\n",
    "    z_score_normal = abs((amount - normal_mean) / normal_std)\n",
    "    \n",
    "    # Invert the Z-scores to get similarity scores\n",
    "    fraud_similarity = 1 / (1 + z_score_fraud)\n",
    "    normal_similarity = 1 / (1 + z_score_normal)\n",
    "    \n",
    "    return fraud_similarity, normal_similarity\n",
    "\n",
    "def process(df):\n",
    "    # Add new features\n",
    "    # Rearrange the rows\n",
    "    df['original_order'] = range(df.shape[0])\n",
    "\n",
    "    df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'], format='%d/%m/%Y %H:%M')\n",
    "    df['dob'] = pd.to_datetime(df['dob'], format='%d/%m/%Y')\n",
    "\n",
    "    df.sort_values(by=['cc_num', 'trans_date_trans_time'], inplace=True)\n",
    "    \n",
    "    # Calculate the time difference between transactions\n",
    "    df['Time_Delta'] = df.groupby('cc_num')['trans_date_trans_time'].diff().dt.total_seconds() / 60.0  # Time delta in minutes\n",
    "    df['Time_Delta'] = df['Time_Delta'].fillna(value=0)\n",
    "    \n",
    "    # Shift the latitude and longitude to get the previous transaction's location\n",
    "    df['prev_lat'] = df.groupby('cc_num')['merch_lat'].shift(1)\n",
    "    df['prev_long'] = df.groupby('cc_num')['merch_long'].shift(1)\n",
    "\n",
    "    # Calculate the distance to the previous transaction\n",
    "    df['distance_to_prev'] = df.apply(\n",
    "        lambda row: calculate_distance2(\n",
    "            {'lat': row['merch_lat'], 'long': row['merch_long']},\n",
    "            {'lat': row['prev_lat'], 'long': row['prev_long']}\n",
    "        ) if not pd.isnull(row['prev_lat']) else None,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    df['distance_to_prev'] = df['distance_to_prev'].fillna(value=0)\n",
    "\n",
    "    # Calculate location consistency as the inverse of the average distance to previous transactions (higher value means more consistency)\n",
    "    df['location_consistency'] = 100 / df.groupby('cc_num')['distance_to_prev'].transform('mean')\n",
    "\n",
    "    # Time-based features\n",
    "    df['hour'] = df['trans_date_trans_time'].dt.hour\n",
    "    df['day_of_week'] = df['trans_date_trans_time'].dt.dayofweek\n",
    "    df['month'] = df['trans_date_trans_time'].dt.month\n",
    "    df['day_of_month'] = df['trans_date_trans_time'].dt.day\n",
    "   \n",
    "    # Age of the account holder\n",
    "    df['age'] = (df['trans_date_trans_time'] - df['dob']).dt.days // 365\n",
    "\n",
    "    df['dist_to_home'] = df.apply(calculate_distance, axis=1)\n",
    "\n",
    "    # Group by category and calculate the mean and standard deviation of transaction amounts\n",
    "    category_stats = df.groupby('category')['amt'].agg(['mean', 'std']).reset_index()\n",
    "    # Merge these stats back into the main dataframe\n",
    "    df = df.merge(category_stats, on='category', how='left')\n",
    "    # Calculate z-score for each transaction amount within its category\n",
    "    df['amt_anomaly_score_cat'] = ((df['amt'] - df['mean']) / df['std'])\n",
    "    df.drop(columns=['mean', 'std'], inplace=True)\n",
    "    # Group by merchant and calculate the mean and standard deviation of transaction amounts\n",
    "    merchant_stats = df.groupby('merchant')['amt'].agg(['mean', 'std']).reset_index()\n",
    "    # Merge these stats back into the main dataframe\n",
    "    df = df.merge(merchant_stats, on='merchant', how='left')\n",
    "    # Calculate z-score for each transaction amount within its merchant\n",
    "    df['amt_anomaly_score_merch'] = ((df['amt'] - df['mean']) / df['std'])\n",
    "    df.drop(columns=['mean', 'std'], inplace=True)\n",
    "\n",
    "    # Calculate the historical average transaction amount for each user\n",
    "    avg_amt_per_user = df.groupby('cc_num')['amt'].transform('mean').rename('avg_amt_per_user')\n",
    "\n",
    "    # Append this feature to the dataset\n",
    "    df['amt_relative_avg'] = (abs(df['amt'] - avg_amt_per_user) / avg_amt_per_user)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=12, random_state=42)\n",
    "\n",
    "    # Create a new column for the cluster labels\n",
    "    df['city_pop_cluster'] = kmeans.fit_predict(df[['city_pop']])\n",
    "\n",
    "    df.drop(columns=['trans_date_trans_time', 'lat', 'long', 'merch_lat', 'merch_long'], inplace=True)\n",
    "    df.drop(columns=['prev_lat', 'prev_long'], inplace=True)\n",
    "\n",
    "    # Identify categorical columns to encode\n",
    "    categorical_cols = ['merchant', 'category', 'gender', 'city', 'state', 'job']\n",
    "\n",
    "    mappings = {}\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    for col in categorical_cols:\n",
    "        df[col] = label_encoder.fit_transform(df[col])\n",
    "        mappings[col] = {label: index for index, label in enumerate(label_encoder.classes_)}\n",
    "\n",
    "\n",
    "    # Calculate the fraud rate by category\n",
    "    fraud_rate_by_category = df.groupby('category')['is_fraud'].mean().reset_index()\n",
    "    fraud_rate_by_category.rename(columns={'is_fraud': 'fraud_rate_cat'}, inplace=True)\n",
    "\n",
    "    # Merge the fraud rate back into the main DataFrame\n",
    "    df = pd.merge(df, fraud_rate_by_category[['category', 'fraud_rate_cat']], on='category', how='left')\n",
    "\n",
    "    # Calculate the fraud rate by merchant\n",
    "    fraud_rate_by_merchant = df.groupby('merchant')['is_fraud'].mean().reset_index()\n",
    "    fraud_rate_by_merchant.rename(columns={'is_fraud': 'fraud_rate_merch'}, inplace=True)\n",
    "\n",
    "    # Merge the fraud rate back into the main DataFrame\n",
    "    df = pd.merge(df, fraud_rate_by_merchant[['merchant', 'fraud_rate_merch']], on='merchant', how='left')\n",
    "\n",
    "    # Separate the transactions\n",
    "    fraud_trans = df[df['is_fraud'] == 1]['amt']\n",
    "    normal_trans = df[df['is_fraud'] == 0]['amt']\n",
    "\n",
    "    # Calculate statistics\n",
    "    fraud_mean, fraud_std = fraud_trans.mean(), fraud_trans.std()\n",
    "    normal_mean, normal_std = normal_trans.mean(), normal_trans.std()\n",
    "\n",
    "    v_calculate_similarity_score = np.vectorize(calculate_similarity_score)\n",
    "\n",
    "    # Apply the function\n",
    "    df['fraud_similarity'], df['normal_similarity'] = v_calculate_similarity_score(\n",
    "        df['amt'],\n",
    "        fraud_mean, fraud_std,\n",
    "        normal_mean, normal_std\n",
    "    )\n",
    "\n",
    "    # Sort the dataset back to its original order\n",
    "    df.sort_values(by='original_order', inplace=True)\n",
    "    df.drop(columns='original_order', inplace=True)\n",
    "    df.drop(columns='cc_num', inplace=True)\n",
    "\n",
    "    return df, mappings\n",
    "\n",
    "trainingSet = pd.read_csv(\"./data/train.csv\")\n",
    "submissionSet = pd.read_csv(\"./data/test.csv\")\n",
    "train_processed, cat_map = process(trainingSet)\n",
    "train_processed.drop(columns=['first', 'last', 'street', 'dob', 'zip', 'trans_num', 'unix_time'], inplace=True)\n",
    "\n",
    "# Merge on Id so that the test set can have feature columns as well\n",
    "test_df = pd.merge(train_processed, submissionSet, left_on='Id', right_on='Id')\n",
    "test_df = test_df.drop(columns=['is_fraud_x'])\n",
    "test_df = test_df.rename(columns={'is_fraud_y': 'is_fraud'})\n",
    "\n",
    "# The training set is where the score is not null\n",
    "train_df = train_processed[train_processed['is_fraud'].notnull()]\n",
    "\n",
    "# Save the datasets with the new features for easy access later\n",
    "test_df.to_csv(\"./data/final_processed_test.csv\", index=False)\n",
    "train_df.to_csv(\"./data/final_processed_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Assuming 'train_df' includes both features and the target ('is_fraud')\n",
    "X = train_df.drop(['is_fraud', 'Id', 'city_pop_cluster'], axis=1)\n",
    "y = train_df['is_fraud']\n",
    "\n",
    "num_cols = ['amt', 'Time_Delta', 'distance_to_prev', 'location_consistency', 'dist_to_home', 'amt_anomaly_score_cat', 'amt_anomaly_score_merch', 'amt_relative_avg', 'fraud_rate_cat', 'fraud_rate_merch', 'fraud_similarity', 'normal_similarity']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
    "test_df[num_cols] = scaler.transform(test_df[num_cols])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9085714285714286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     96876\n",
      "         1.0       0.98      0.85      0.91       375\n",
      "\n",
      "    accuracy                           1.00     97251\n",
      "   macro avg       0.99      0.92      0.95     97251\n",
      "weighted avg       1.00      1.00      1.00     97251\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, make_scorer\n",
    "# Initialize the XGBClassifier\n",
    "xgb_clf = XGBClassifier(colsample_bytree=0.9, learning_rate=0.1, max_depth=5, n_estimators=900, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgb = xgb_clf.predict(X_val)\n",
    "print(f1_score(y_val, y_pred_xgb))\n",
    "print(classification_report(y_val, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9085714285714286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     96876\n",
      "         1.0       0.98      0.85      0.91       375\n",
      "\n",
      "    accuracy                           1.00     97251\n",
      "   macro avg       0.99      0.92      0.95     97251\n",
      "weighted avg       1.00      1.00      1.00     97251\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, make_scorer\n",
    "# Initialize the XGBClassifier\n",
    "xgb_clf2 = XGBClassifier(colsample_bytree=0.9, learning_rate=0.15, max_depth=None, n_estimators=900, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "xgb_clf2.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgb2 = xgb_clf2.predict(X_val)\n",
    "print(f1_score(y_val, y_pred_xgb2))\n",
    "print(classification_report(y_val, y_pred_xgb2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building estimator 1 of 10 for this parallel run (total 100)...\n",
      "Building estimator 1 of 10 for this parallel run (total 100)...\n",
      "Building estimator 1 of 10 for this parallel run (total 100)...\n",
      "Building estimator 1 of 10 for this parallel run (total 100)...\n",
      "Building estimator 1 of 10 for this parallel run (total 100)...\n",
      "Building estimator 1 of 10 for this parallel run (total 100)...\n",
      "Building estimator 1 of 10 for this parallel run (total 100)...\n",
      "Building estimator 1 of 10 for this parallel run (total 100)...\n",
      "Building estimator 1 of 10 for this parallel run (total 100)...\n",
      "Building estimator 1 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:  3.7min remaining:  8.6min\n",
      "[Parallel(n_jobs=10)]: Done   7 out of  10 | elapsed:  3.7min remaining:  1.6min\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:  3.7min finished\n",
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:   15.3s remaining:   35.8s\n",
      "[Parallel(n_jobs=10)]: Done   7 out of  10 | elapsed:   15.7s remaining:    6.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8694362017804155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:   16.3s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "# Create a VotingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Initialize the Bagging Classifier\n",
    "bagging_clf = BaggingClassifier(estimator=xgb_clf, n_estimators=100, random_state=42, n_jobs=-1, verbose=3)\n",
    "\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "y_pred_bag = bagging_clf.predict(X_val)\n",
    "print(f1_score(y_val, y_pred_bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8115107913669065\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00     96876\n",
      "         1.0       0.88      0.75      0.81       375\n",
      "\n",
      "    accuracy                           1.00     97251\n",
      "   macro avg       0.94      0.88      0.91     97251\n",
      "weighted avg       1.00      1.00      1.00     97251\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "# Create a VotingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "rf1 = RandomForestClassifier(class_weight='balanced_subsample', n_estimators=300, max_depth=None, min_samples_split=10, min_samples_leaf=4, criterion='entropy', n_jobs=-1, random_state=42)\n",
    "\n",
    "rf1.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf1 = rf1.predict(X_val)\n",
    "print(f1_score(y_val, y_pred_rf1))\n",
    "print(classification_report(y_val, y_pred_rf1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building estimator 1 of 10 for this parallel run (total 100)...\n",
      "Building estimator 1 of 10 for this parallel run (total 100)...Building estimator 1 of 10 for this parallel run (total 100)...\n",
      "\n",
      "Building estimator 1 of 10 for this parallel run (total 100)...Building estimator 1 of 10 for this parallel run (total 100)...\n",
      "Building estimator 1 of 10 for this parallel run (total 100)...\n",
      "\n",
      "Building estimator 1 of 10 for this parallel run (total 100)...Building estimator 1 of 10 for this parallel run (total 100)...\n",
      "\n",
      "Building estimator 1 of 10 for this parallel run (total 100)...\n",
      "Building estimator 1 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:  3.9min remaining:  9.2min\n",
      "[Parallel(n_jobs=10)]: Done   7 out of  10 | elapsed:  4.0min remaining:  1.7min\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:  4.0min finished\n",
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:   15.8s remaining:   36.9s\n",
      "[Parallel(n_jobs=10)]: Done   7 out of  10 | elapsed:   16.3s remaining:    7.0s\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:   16.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.888243831640058\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "# Create a VotingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('rf1', rf1), ('bag', bagging_clf), ('xgb', xgb_clf), ('xgb2', xgb_clf2)],\n",
    "    voting='soft',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the ensemble model\n",
    "vot_clf = voting_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_vote = vot_clf.predict(X_val)\n",
    "f1_score_vote = f1_score(y_val, y_pred_vote)\n",
    "print(f1_score_vote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building estimator 1 of 10 for this parallel run (total 100)...\n",
      "Building estimator 1 of 10 for this parallel run (total 100)...\n",
      "Building estimator 1 of 10 for this parallel run (total 100)...Building estimator 1 of 10 for this parallel run (total 100)...\n",
      "Building estimator 1 of 10 for this parallel run (total 100)...Building estimator 1 of 10 for this parallel run (total 100)...\n",
      "\n",
      "Building estimator 1 of 10 for this parallel run (total 100)...\n",
      "Building estimator 1 of 10 for this parallel run (total 100)...\n",
      "Building estimator 1 of 10 for this parallel run (total 100)...\n",
      "\n",
      "Building estimator 1 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 2 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 3 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 4 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 5 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 6 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 7 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 8 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 9 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n",
      "Building estimator 10 of 10 for this parallel run (total 100)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:  3.9min remaining:  9.1min\n",
      "[Parallel(n_jobs=10)]: Done   7 out of  10 | elapsed:  3.9min remaining:  1.7min\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:  3.9min finished\n",
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:   15.1s remaining:   35.2s\n",
      "[Parallel(n_jobs=10)]: Done   7 out of  10 | elapsed:   15.9s remaining:    6.8s\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:   16.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8859649122807017\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "# Create a VotingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_clf2 = VotingClassifier(\n",
    "    estimators=[('xgb', xgb_clf), ('bag', bagging_clf), ('rf1', rf1)],\n",
    "    voting='soft',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the ensemble model\n",
    "vot_clf2 = voting_clf2.fit(X_train, y_train)\n",
    "\n",
    "y_pred_vote2 = vot_clf2.predict(X_val)\n",
    "f1_score_vote2 = f1_score(y_val, y_pred_vote2)\n",
    "print(f1_score_vote2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:   10.9s remaining:   25.4s\n",
      "[Parallel(n_jobs=10)]: Done   7 out of  10 | elapsed:   11.4s remaining:    4.9s\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:   11.7s finished\n"
     ]
    }
   ],
   "source": [
    "# Create Kaggle Submission\n",
    "pred = test_df.drop(['is_fraud', 'city_pop_cluster', 'Id'], axis=1)\n",
    "pred2 = test_df.drop(['is_fraud'], axis=1)\n",
    "\n",
    "pred2['is_fraud'] = vot_clf.predict(pred)\n",
    "pred2.is_fraud = pred2.is_fraud.astype(int)\n",
    "submission = pred2[['Id', 'is_fraud']]\n",
    "submission.to_csv(\"./data/competition_submission_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:   11.7s remaining:   27.4s\n",
      "[Parallel(n_jobs=10)]: Done   7 out of  10 | elapsed:   12.0s remaining:    5.2s\n",
      "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:   12.3s finished\n"
     ]
    }
   ],
   "source": [
    "# Create Kaggle Submission\n",
    "pred3 = test_df.drop(['is_fraud'], axis=1)\n",
    "\n",
    "pred3['is_fraud'] = vot_clf2.predict(pred)\n",
    "pred3.is_fraud = pred3.is_fraud.astype(int)\n",
    "submission2 = pred3[['Id', 'is_fraud']]\n",
    "submission2.to_csv(\"./data/competition_submission_final2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('competitioin_model_final.obj', 'wb') as f:\n",
    "    pickle.dump(vot_clf, f)\n",
    "\n",
    "with open('competitioin_model_final2.obj', 'wb') as f:\n",
    "    pickle.dump(vot_clf2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Kaggle Submission\n",
    "pred4 = test_df.drop(['is_fraud'], axis=1)\n",
    "\n",
    "pred4['is_fraud'] = xgb_clf.predict(pred)\n",
    "pred4.is_fraud = pred4.is_fraud.astype(int)\n",
    "submission2 = pred4[['Id', 'is_fraud']]\n",
    "submission2.to_csv(\"./data/competition_submission_final3.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
