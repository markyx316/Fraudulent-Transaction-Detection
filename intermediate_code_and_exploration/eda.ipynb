{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from geopy.distance import geodesic\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Calculate distance from home function\n",
    "def calculate_distance(row):\n",
    "    home_location = (row['lat'], row['long'])\n",
    "    merch_location = (row['merch_lat'], row['merch_long'])\n",
    "    return geodesic(home_location, merch_location).miles\n",
    "\n",
    "# Function to calculate distance between two points\n",
    "def calculate_distance2(row1, row2):\n",
    "    point1 = (row1['lat'], row1['long'])\n",
    "    point2 = (row2['lat'], row2['long'])\n",
    "    return geodesic(point1, point2).miles\n",
    "\n",
    "def calculate_similarity_score(amount, fraud_mean, fraud_std, normal_mean, normal_std):\n",
    "    # Calculate Z-scores for fraud and normal\n",
    "    z_score_fraud = abs((amount - fraud_mean) / fraud_std)\n",
    "    z_score_normal = abs((amount - normal_mean) / normal_std)\n",
    "    \n",
    "    # Invert the Z-scores to get similarity scores\n",
    "    fraud_similarity = 1 / (1 + z_score_fraud)\n",
    "    normal_similarity = 1 / (1 + z_score_normal)\n",
    "    \n",
    "    return fraud_similarity, normal_similarity\n",
    "\n",
    "def process(df):\n",
    "    # Add new features\n",
    "    # Rearrange the rows\n",
    "    df['original_order'] = range(df.shape[0])\n",
    "\n",
    "    df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'], format='%d/%m/%Y %H:%M')\n",
    "    df['dob'] = pd.to_datetime(df['dob'], format='%d/%m/%Y')\n",
    "\n",
    "    # df['timestamp'] = pd.to_datetime(df['trans_date_trans_time'], format='%d/%m/%Y %H:%M')\n",
    "    # df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'], format='%d/%m/%Y %H:%M')\n",
    "\n",
    "    df.sort_values(by=['cc_num', 'trans_date_trans_time'], inplace=True)\n",
    "    # Calculate time difference in hours\n",
    "    # df['time_since_last_trans'] = df.groupby('cc_num')['trans_date_trans_time'].diff().apply(lambda x: x.total_seconds() / 3600)\n",
    "    # # Fill NaN values for each user's first transaction\n",
    "    # df['time_since_last_trans'] = df['time_since_last_trans'].fillna(value=0)\n",
    "    # Calculate the time difference between transactions\n",
    "    df['Time_Delta'] = df.groupby('cc_num')['trans_date_trans_time'].diff().dt.total_seconds() / 60.0  # Time delta in minutes\n",
    "    df['Time_Delta'] = df['Time_Delta'].fillna(value=0)\n",
    "    \n",
    "    # Shift the latitude and longitude to get the previous transaction's location\n",
    "    df['prev_lat'] = df.groupby('cc_num')['merch_lat'].shift(1)\n",
    "    df['prev_long'] = df.groupby('cc_num')['merch_long'].shift(1)\n",
    "\n",
    "    # Calculate the distance to the previous transaction\n",
    "    df['distance_to_prev'] = df.apply(\n",
    "        lambda row: calculate_distance2(\n",
    "            {'lat': row['merch_lat'], 'long': row['merch_long']},\n",
    "            {'lat': row['prev_lat'], 'long': row['prev_long']}\n",
    "        ) if not pd.isnull(row['prev_lat']) else None,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    df['distance_to_prev'] = df['distance_to_prev'].fillna(value=0)\n",
    "\n",
    "    # Calculate location consistency as the inverse of the average distance to previous transactions (higher value means more consistency)\n",
    "    df['location_consistency'] = 100 / df.groupby('cc_num')['distance_to_prev'].transform('mean')\n",
    "\n",
    "    # Time-based features\n",
    "    df['hour'] = df['trans_date_trans_time'].dt.hour\n",
    "    df['day_of_week'] = df['trans_date_trans_time'].dt.dayofweek\n",
    "   \n",
    "    # Age of the account holder\n",
    "    df['age'] = (df['trans_date_trans_time'] - df['dob']).dt.days // 365\n",
    "\n",
    "    # df['trans_dist'] = haversine_distance(df['lat'], df['long'], df['merch_lat'], df['merch_long'])\n",
    "    df['dist_to_home'] = df.apply(calculate_distance, axis=1)\n",
    "\n",
    "    # Group by category and calculate the mean and standard deviation of transaction amounts\n",
    "    category_stats = df.groupby('category')['amt'].agg(['mean', 'std']).reset_index()\n",
    "    # Merge these stats back into the main dataframe\n",
    "    df = df.merge(category_stats, on='category', how='left')\n",
    "    # Calculate z-score for each transaction amount within its category\n",
    "    df['amt_anomaly_score_cat'] = ((df['amt'] - df['mean']) / df['std'])\n",
    "    df.drop(columns=['mean', 'std'], inplace=True)\n",
    "    # Group by merchant and calculate the mean and standard deviation of transaction amounts\n",
    "    merchant_stats = df.groupby('merchant')['amt'].agg(['mean', 'std']).reset_index()\n",
    "    # Merge these stats back into the main dataframe\n",
    "    df = df.merge(merchant_stats, on='merchant', how='left')\n",
    "    # Calculate z-score for each transaction amount within its merchant\n",
    "    df['amt_anomaly_score_merch'] = ((df['amt'] - df['mean']) / df['std'])\n",
    "    df.drop(columns=['mean', 'std'], inplace=True)\n",
    "\n",
    "    # merch_stats = df.groupby('merchant')['amt'].agg(['mean', 'std']).reset_index(names=['merch_mean', 'merch_std'])\n",
    "    # # Merge these stats back into the main dataframe\n",
    "    # df = df.merge(merch_stats, on='merchant', how='left')\n",
    "    # # Calculate z-score for each transaction amount within its merchant\n",
    "    # df['amt_anomaly_score_merch'] = (df['amt'] - df['merch_mean']) / df['merch_std']\n",
    "\n",
    "    # user_avg_amt = df.groupby('cc_num')['amt'].mean().reset_index(name='Avg_Amt')\n",
    "    # df = df.merge(user_avg_amt, on='cc_num')\n",
    "    # df['Relative_Amt'] = abs(df['amt'] - df['Avg_Amt']) / df['Avg_Amt']\n",
    "    # Calculate the historical average transaction amount for each user\n",
    "    avg_amt_per_user = df.groupby('cc_num')['amt'].transform('mean').rename('avg_amt_per_user')\n",
    "\n",
    "    # Append this feature to the dataset\n",
    "    df['amt_relative_avg'] = (abs(df['amt'] - avg_amt_per_user) / avg_amt_per_user)\n",
    "    #df['relative_amt'] = abs(df['amt'] - avg_amt_per_user) / avg_amt_per_user\n",
    "\n",
    "    kmeans = KMeans(n_clusters=12, random_state=42)\n",
    "\n",
    "    # Create a new column for the cluster labels\n",
    "    df['city_pop_cluster'] = kmeans.fit_predict(df[['city_pop']])\n",
    "\n",
    "\n",
    "    df.drop(columns=['trans_date_trans_time', 'lat', 'long', 'merch_lat', 'merch_long'], inplace=True)\n",
    "    df.drop(columns=['prev_lat', 'prev_long', 'cc_num'], inplace=True)\n",
    "\n",
    "    # Identify categorical columns to encode\n",
    "    categorical_cols = ['merchant', 'category', 'gender', 'city', 'state', 'job']\n",
    "\n",
    "    mappings = {}\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    for col in categorical_cols:\n",
    "        df[col] = label_encoder.fit_transform(df[col])\n",
    "        mappings[col] = {label: index for index, label in enumerate(label_encoder.classes_)}\n",
    "\n",
    "\n",
    "    # Calculate the fraud rate by category\n",
    "    fraud_rate_by_category = df.groupby('category')['is_fraud'].mean().reset_index()\n",
    "    fraud_rate_by_category.rename(columns={'is_fraud': 'fraud_rate_cat'}, inplace=True)\n",
    "\n",
    "    # Merge the normalized fraud rate back into the main DataFrame\n",
    "    df = pd.merge(df, fraud_rate_by_category[['category', 'fraud_rate_cat']], on='category', how='left')\n",
    "\n",
    "    # # Determine the threshold for high fraud risk categories\n",
    "    # threshold = fraud_rate_by_category['fraud_rate'].quantile(0.75)\n",
    "\n",
    "    # # Identify high fraud risk categories\n",
    "    # high_risk_categories = fraud_rate_by_category[fraud_rate_by_category['fraud_rate'] >= threshold]['category']\n",
    "\n",
    "    # # Display high-risk categories\n",
    "    # print('High-risk categories:')\n",
    "    # print(high_risk_categories)\n",
    "\n",
    "    # Calculate the fraud rate by merchant\n",
    "    fraud_rate_by_merchant = df.groupby('merchant')['is_fraud'].mean().reset_index()\n",
    "    fraud_rate_by_merchant.rename(columns={'is_fraud': 'fraud_rate_merch'}, inplace=True)\n",
    "\n",
    "    # Merge the normalized fraud rate back into the main DataFrame\n",
    "    df = pd.merge(df, fraud_rate_by_merchant[['merchant', 'fraud_rate_merch']], on='merchant', how='left')\n",
    "\n",
    "    # Separate the transactions\n",
    "    fraud_trans = df[df['is_fraud'] == 1]['amt']\n",
    "    normal_trans = df[df['is_fraud'] == 0]['amt']\n",
    "\n",
    "    # Calculate statistics\n",
    "    fraud_mean, fraud_std = fraud_trans.mean(), fraud_trans.std()\n",
    "    normal_mean, normal_std = normal_trans.mean(), normal_trans.std()\n",
    "\n",
    "    v_calculate_similarity_score = np.vectorize(calculate_similarity_score)\n",
    "\n",
    "    # Apply the function\n",
    "    df['fraud_similarity'], df['normal_similarity'] = v_calculate_similarity_score(\n",
    "        df['amt'],\n",
    "        fraud_mean, fraud_std,\n",
    "        normal_mean, normal_std\n",
    "    )\n",
    "\n",
    "    # # Determine the merch_threshold for high fraud risk categories\n",
    "    # merch_threshold = fraud_rate_by_merchant['fraud_rate'].quantile(0.75)\n",
    "\n",
    "    # # Identify high fraud risk categories\n",
    "    # high_risk_merchants = fraud_rate_by_merchant[fraud_rate_by_merchant['fraud_rate'] >= merch_threshold]['merchant']\n",
    "\n",
    "    # # Display high-risk merchants\n",
    "    # print('High-risk merchants:')\n",
    "    # print(high_risk_merchants)\n",
    "    # # Initialize the new feature with 0\n",
    "    # df['high_risk_category'] = 0\n",
    "    # df['high_risk_merchant'] = 0\n",
    "\n",
    "    # # Flag transactions in high-risk categories\n",
    "    # df.loc[df['category'].isin(high_risk_categories), 'high_risk_category'] = 1\n",
    "    # df.loc[df['merchant'].isin(high_risk_merchants), 'high_risk_merchant'] = 1\n",
    "\n",
    "    # Sort the dataset back to its original order\n",
    "    df.sort_values(by='original_order', inplace=True)\n",
    "    df.drop(columns='original_order', inplace=True)\n",
    "\n",
    "    return df, mappings\n",
    "\n",
    "trainingSet = pd.read_csv(\"./data/train.csv\")\n",
    "submissionSet = pd.read_csv(\"./data/test.csv\")\n",
    "train_processed, cat_map = process(trainingSet)\n",
    "train_processed.drop(columns=['first', 'last', 'street', 'dob', 'zip', 'trans_num', 'unix_time'], inplace=True)\n",
    "\n",
    "# Merge on Id so that the test set can have feature columns as well\n",
    "test_df = pd.merge(train_processed, submissionSet, left_on='Id', right_on='Id')\n",
    "test_df = test_df.drop(columns=['is_fraud_x'])\n",
    "test_df = test_df.rename(columns={'is_fraud_y': 'is_fraud'})\n",
    "\n",
    "# The training set is where the score is not null\n",
    "train_df = train_processed[train_processed['is_fraud'].notnull()]\n",
    "\n",
    "# Save the datasets with the new features for easy access later\n",
    "test_df.to_csv(\"./data/test_p.csv\", index=False)\n",
    "train_df.to_csv(\"./data/train_p.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original category values are:\n",
      "grocery_pos\n",
      "misc_net\n",
      "shopping_net\n",
      "shopping_pos\n"
     ]
    }
   ],
   "source": [
    "# Assuming the prediction for 'category' was 2\n",
    "predicted = [4, 8, 11, 12]\n",
    "col_name = 'category'\n",
    "\n",
    "# Find the original category from the mapping\n",
    "print(f\"The original category values are:\")\n",
    "for pred in predicted:\n",
    "    original_category = {v: k for k, v in cat_map[col_name].items()}[pred]\n",
    "    print(original_category)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
